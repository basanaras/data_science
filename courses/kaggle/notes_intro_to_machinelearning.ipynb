{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "def208b6",
   "metadata": {},
   "source": [
    "## Intro to Machine Learning (Kaggle)  \n",
    "#### Notes taken by: Sara Nasab / Github: @basanaras\n",
    "Caution: There is no source dataset that I am extracting from. Running the kernel will result in errors.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61198dc",
   "metadata": {},
   "source": [
    "### (1) Importing relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedba511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library using convential abbreviation\n",
    "import pandas as pd\n",
    "file = '../input/..'\n",
    "data = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccdca05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To print summary of data\n",
    "data.describe()\n",
    "\n",
    "# To print the first five lines of dataframe: \n",
    "data.head() \n",
    "# Pro tip: To change the number N of lines shown, insert N in parentheses\n",
    "\n",
    "# List column headers (used to see what kind of data is stored in df):\n",
    "data.columns "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277948dd",
   "metadata": {},
   "source": [
    "Extract data to make predictions: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f60879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose features of interest that factor in the prediction target \n",
    "y = data.Price\n",
    "\n",
    "features = ['Rooms', 'Bathroom', 'Size']\n",
    "X = data[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a17f08",
   "metadata": {},
   "source": [
    "### (2) Creating a decision tree model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c019ffed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use decision tree from the scikit-learn library\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define model. The decision tree is initialized with a random_state to ensure reproducibility\n",
    "data_model = DecisionTreeRegressor(random_state = 1)\n",
    "\n",
    "# Fit model \n",
    "data_model.fit(X, y)\n",
    "\n",
    "# Predictions of y using X\n",
    "data_model.predict(X)\n",
    "data_model.predict(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06faf8e",
   "metadata": {},
   "source": [
    "### (3) Model validation: How much can we trust the model that we created? \n",
    "* Avoid making predictions with the *training data* and compare predictions to the *training data* \n",
    "* We can measure predictive accuracy different ways. One useful metric is the **Mean Absolute Error (MAE)** \n",
    "$$ \\text{MAE} = \\frac{\\sum^{n}_{i=1} | y_i - x_i |}{n} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cc135e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use built-in MAE function from scikit-learn\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_from_model = data.predict(X)\n",
    "mean_absolute_error(y, predicted_from_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d4d1b6",
   "metadata": {},
   "source": [
    "This is an \"in-sample\" score, i.e. using the predictions from the training model to calculate MAE. Most likely the model will perform poorly in practice (on new data).\n",
    "\n",
    "One way to get around this: Exclude some data points from the training data, and use that to test the model's accuracy. This set of training points is called **validation data**. \n",
    "\n",
    "We can use the **validation data** to compute the MAE, and ultimately measure the accuracy of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6f8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input requires X and y, and then returns split training and testing/validation data \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=0) # Insert random state \n",
    "\n",
    "# Create model \n",
    "data_model = DecisionTreeRegressor() \n",
    "# Fit model \n",
    "data_model.fit(train_X, train_y) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bb6a0f",
   "metadata": {},
   "source": [
    "Just like before, we fit the model to the training data. *However* this time, the fit is only done to a subset of the available data, renamed as `train_X` and `train_y`. \n",
    "\n",
    "We can obtain the *predicted* data from the *validation* data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8ad894",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = data_model.predict(val_X)\n",
    "\n",
    "# Calculate MAE correctly! \n",
    "mean_absolute_error(val_y, val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90d6245",
   "metadata": {},
   "source": [
    "### (4) Experimenting with different models: Overfitting vs. underfitting \n",
    "\n",
    "In the context of decision trees that are composed of splits (levels) and the leaves, we say that the *deeper* the tree is, the more splits the tree has (and the *shallower* the tree, the fewer splots the tree has. \n",
    "\n",
    "* **Overfitting**: When the tree has many splits (many leaves), the model fits the training set very well. Often captures outliers that are not indicative of new datasets.\n",
    "* **Underfitting**: When the tree is too shallow (fewer leaves), the likelihood of recognizing distinct patterns in the data is smaller. \n",
    "\n",
    "Both cases are likely to result in **poor predictions** on both the validation and new data sets.\n",
    "\n",
    "We can often avoid these issues by varying the looking at how the number of leaves influences the MAE of our model using `max_leaf_nodes`: \n",
    "\n",
    "`DecisionTreeRegressor(max_leaf_nodes = n, random_state = 0)`\n",
    "\n",
    "We can take advantage of varying the depth and compare different models to choose the appropriate one that minimizes the MAE.\n",
    "\n",
    "**Note: We use the *validation* data to test the model!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18edbad0",
   "metadata": {},
   "source": [
    "#### In practice: \n",
    "1. Find the appropriate `max_leaf_nodes` by comparing MAEs on the **training set**. \n",
    "2. Once the optimal `max_leaf_nodes` are found, then fit the data on the **entire** dataset. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
